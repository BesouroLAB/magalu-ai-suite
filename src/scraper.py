"""
Scraper inteligente usando Gemini Google Search + URL Context.
UsuÃ¡rio informa apenas o cÃ³digo do produto Magalu.
O Gemini pesquisa no Google, encontra a pÃ¡gina real e extrai os dados.
"""
import os
import re
import requests
from bs4 import BeautifulSoup
from google import genai
from google.genai.types import Tool, GenerateContentConfig
from dotenv import load_dotenv

load_dotenv()

EXTRACTION_PROMPT = """
VocÃª Ã© um extrator de dados de produtos do Magazine Luiza (Magalu).

TAREFA: Pesquise o produto com o cÃ³digo "{code}" no site magazineluiza.com.br e extraia os dados abaixo.

**FORMATO DE SAÃDA OBRIGATÃ“RIO:**
TÃTULO DO PRODUTO: [tÃ­tulo completo do produto]

MARCA: [marca/fabricante]

DESCRIÃ‡ÃƒO DO FABRICANTE:
[descriÃ§Ã£o completa do produto, mÃ¡ximo 1500 caracteres]

FICHA TÃ‰CNICA PRINCIPAL:
- [EspecificaÃ§Ã£o 1]: [Valor]
- [EspecificaÃ§Ã£o 2]: [Valor]
- [EspecificaÃ§Ã£o 3]: [Valor]
[...continue com todas as specs disponÃ­veis, mÃ¡ximo 20]

PREÃ‡O: [se disponÃ­vel]

**REGRAS RIGOROSAS DE FONTE DE DADOS:**
1. Pesquise primariamente e especificamente por: site:magazineluiza.com.br "{code}".
2. Se a ficha da Magalu nÃ£o possuir os dados tÃ©cnicos necessÃ¡rios, vocÃª pode usar a ferramenta de busca para procurar **EXCLUSIVAMENTE no site oficial do FABRICANTE**. 
3. Ã‰ TERMINANTEMENTE PROIBIDO extrair dados de concorrentes (Amazon, Mercado Livre, Casas Bahia, etc).
4. Se vocÃª extrair qualquer informaÃ§Ã£o do site do fabricante (fora da Magalu), adicione uma linha no final da sua resposta exatamente assim: "FONTE EXTERNA: [coloque a URL do fabricante aqui]".
5. Extraia SOMENTE dados reais do produto encontrado. NÃƒO invente informaÃ§Ãµes.
6. Se algum dado nÃ£o estiver disponÃ­vel sequer no fabricante, escreva "NÃ£o informado".
7. Foque nas especificaÃ§Ãµes tÃ©cnicas mais relevantes para um roteiro de vÃ­deo (dimensÃµes, peso, voltagem, materiais, capacidades).
"""


def scrape_with_gemini(code_or_url: str) -> dict:
    """
    Extrai dados de produto do Magalu (Texto + Imagem).

    Args:
        code_or_url: CÃ³digo do produto (ex: '240304700') ou URL completa.

    Returns:
        DicionÃ¡rio com 'text' (dados estruturados) e 'images' (lista de dicts com 'bytes' e 'mime').
    """
    api_key = os.environ.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        return {"text": "âŒ GEMINI_API_KEY nÃ£o configurada. Configure no painel lateral.", "images": []}

    input_val = code_or_url.strip()

    # Se for URL completa, tenta extrair o cÃ³digo dela
    if input_val.startswith("http"):
        match = re.search(r'/p/(\w+)', input_val)
        code = match.group(1) if match else input_val
    else:
        code = re.sub(r'[^0-9a-zA-Z]', '', input_val)

    prompt = EXTRACTION_PROMPT.replace("{code}", code)

    # Tenta extrair as imagens da galeria do produto
    images_list = _extract_images(code)
    
    result_text = None
    # MÃ©todo 1: Google Search + URL Context combinados (mais poderoso)
    result = _try_combined_search(prompt, api_key)
    if result:
        result_text = result
    else:
        # MÃ©todo 2: Apenas Google Search
        result = _try_google_search(prompt, api_key)
        if result:
            result_text = result

    if not result_text:
        result_text = f"âš ï¸ NÃ£o foi possÃ­vel extrair dados do produto {code}.\nCole a ficha tÃ©cnica manualmente."

    return {
        "text": result_text,
        "images": images_list
    }

def _extract_images(code: str, max_images: int = 5):
    """Tenta baixar imagens da galeria do produto."""
    images = []
    try:
        url = f"https://www.magazineluiza.com.br/p/{code}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            img_urls = []
            
            # 1. Tenta a og:image primeiro (garante pelo menos a principal)
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                img_urls.append(og_image['content'])
                
            # 2. Varrer cÃ³digo por imagens de showcase/produto (regex)
            encontradas = re.findall(r'https://[^"\'\s]+\.(?:jpg|jpeg|png|webp)', response.text)
            
            for img in encontradas:
                if 'showcase' in img or 'produto' in img or 'magazineluiza.com.br' in img:
                    # Limpeza extra para evitar Ã­cones minÃºsculos e links quebrados
                    if 'thumb' not in img.lower() and 'icon' not in img.lower():
                        img_urls.append(img)
            
            # Remove duplicatas mantendo a ordem (og:image primeiro)
            seen = set()
            unique_urls = [x for x in img_urls if not (x in seen or seen.add(x))]
            
            for img_url in unique_urls[:max_images]:
                try:
                    img_response = requests.get(img_url, timeout=5)
                    if img_response.status_code == 200:
                        mime = img_response.headers.get('content-type', 'image/jpeg')
                        images.append({"bytes": img_response.content, "mime": mime})
                except Exception:
                    continue
                    
    except Exception as e:
        print(f"[scraper] Erro ao extrair imagens do produto {code}: {e}")
        
    return images


def _try_combined_search(prompt: str, api_key: str) -> str | None:
    """Tenta com Google Search + URL Context combinados."""
    try:
        client = genai.Client(api_key=api_key)
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(
                tools=[
                    Tool(google_search={}),
                    Tool(url_context={}),
                ],
            ),
        )
        text = response.text
        if text and len(text.strip()) > 80:
            return text
    except Exception as e:
        print(f"[scraper] Combinado falhou: {e}")
    return None


def _try_google_search(prompt: str, api_key: str) -> str | None:
    """Fallback: apenas Google Search grounding."""
    try:
        client = genai.Client(api_key=api_key)
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(
                tools=[Tool(google_search={})],
            ),
        )
        text = response.text
        if text and len(text.strip()) > 80:
            return text
    except Exception as e:
        print(f"[scraper] Google Search falhou: {e}")
    return None


def parse_codes(raw_input: str) -> list[str]:
    """
    Parseia a entrada do usuÃ¡rio em lista de cÃ³digos.
    Aceita: vÃ­rgula, espaÃ§o, nova linha como separador.
    """
    codes = re.split(r'[,\s\n]+', raw_input.strip())
    return [c.strip() for c in codes if c.strip()]


if __name__ == "__main__":
    raw = input("Digite cÃ³digo(s) de produto (separados por vÃ­rgula): ")
    codes = parse_codes(raw)
    for code in codes:
        print(f"\nğŸ” Pesquisando produto {code} no Magalu...")
        print(scrape_with_gemini(code))
        print("\n" + "=" * 60)
