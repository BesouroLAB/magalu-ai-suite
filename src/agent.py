import os
import json
import glob
from google import genai
from google.genai.types import GenerateContentConfig
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

PROJECT_ROOT = os.path.join(os.path.dirname(__file__), '..')

# Tabela de pre√ßos por 1M tokens (USD)
PRICING_USD_PER_1M = {
    "gemini-2.5-flash": {"input": 0.70, "output": 2.10},
    "gemini-2.5-pro":   {"input": 3.50, "output": 10.50},
    "gemini-1.5-flash":  {"input": 0.35, "output": 1.05},
    # Novos modelos (Z.ai, Kimi, etc. em modo free por enquanto)
    "gpt-4o-mini": {"input": 0.00, "output": 0.00},
    "x-ai/grok-4-1-fast": {"input": 0.00, "output": 0.00},
    "x-ai/grok-2": {"input": 0.00, "output": 0.00},
    "moonshot-v1-8k": {"input": 0.00, "output": 0.00},
    "glm-4-flash": {"input": 0.00, "output": 0.00},
    "deepseek/deepseek-r1-0528:free": {"input": 0.00, "output": 0.00},
    "deepseek/deepseek-r1-0528:free": {"input": 0.00, "output": 0.00},
    "google/gemma-3-27b-it:free": {"input": 0.00, "output": 0.00},
    "meta-llama/llama-4-scout:free": {"input": 0.00, "output": 0.00},
    "meta-llama/llama-3.1-70b-instruct": {"input": 0.00, "output": 0.00},
    "claude-3-5-sonnet": {"input": 0.00, "output": 0.00},
}
USD_TO_BRL = 5.80

MODELOS_DISPONIVEIS = {
    "‚ö° Gemini 2.5 Flash [PAGO] ‚Äî ~R$0,03/roteiro": "gemini-2.5-flash",
    "üèÜ Gemini 2.5 Pro [PAGO] ‚Äî ~R$0,06/roteiro": "gemini-2.5-pro",
    "üî• Grok 4.1 Fast [GR√ÅTIS] ‚Äî Criativo (Puter)": "puter/x-ai/grok-4-1-fast",
    "üêã DeepSeek R1 [GR√ÅTIS] ‚Äî T√©cnico (OpenRouter)": "openrouter/deepseek/deepseek-r1-0528:free",
    "ü§ñ GPT-4o Mini [GR√ÅTIS] ‚Äî Flu√≠do (OpenAI)": "openai/gpt-4o-mini",
    "üß† DeepSeek R1 [GR√ÅTIS] ‚Äî An√°lise (OpenRouter)": "openrouter/deepseek/deepseek-r1-0528:free",
    "üí∞ Gemini 1.5 Flash [GR√ÅTIS/PAGO] ‚Äî Super Econ√¥mico": "gemini-1.5-flash",
    "üî• Grok 2 [GR√ÅTIS] ‚Äî Robusto (Puter)": "puter/x-ai/grok-2",
    "üíé Gemma 3 27B [GR√ÅTIS] ‚Äî Multimodal (OpenRouter)": "openrouter/google/gemma-3-27b-it:free",
    "ü¶ô Llama 4 Scout [GR√ÅTIS] ‚Äî Nova Gera√ß√£o (OpenRouter)": "openrouter/meta-llama/llama-4-scout:free",
    "üá®üá≥ GLM-4 Flash [GR√ÅTIS] ‚Äî Ficha T√©cnica (Z.ai)": "zai/glm-4-flash",
    "üåô Kimi v1 [GR√ÅTIS] ‚Äî Coer√™ncia (Moonshot)": "kimi/moonshot-v1-8k",
    "ü¶ô Llama 3.1 70B [GR√ÅTIS] ‚Äî Equilibrado (Puter)": "puter/meta-llama/llama-3.1-70b-instruct",
    "üé≠ Claude 3.5 Sonnet [GR√ÅTIS] ‚Äî Narrativa Premium (Puter)": "puter/claude-3-5-sonnet",
}

MODELOS_DESCRICAO = {
    "gemini-2.5-flash": "[RECOMENDADO] (2025) O equil√≠brio perfeito. Extremamente r√°pido, lida bem com lotes e tem a melhor integra√ß√£o com a persona da Lu. Custo baix√≠ssimo (~R$ 0,03).",
    "gemini-2.5-pro": "[ELITE] (2025) O modelo mais inteligente. Ideal para produtos complexos ou roteiros que exigem criatividade fora da curva e l√≥gica impec√°vel. Custo (~R$ 0,06).",
    "gemini-1.5-flash": "[ECON√îMICO] (2024) Uma vers√£o est√°vel e muito r√°pida se as chaves 2.5 estiverem lentas. √ìtimo custo-benef√≠cio.",
    "openai/gpt-4o-mini": "[EST√ÅVEL] (2024) Respostas muito diretas e limpas. Excelente para manter o formato NW sem erros de estrutura.",
    "puter/x-ai/grok-4-1-fast": "[NEGOCIAL/RETIRO] (2025) Excelente para Reels e formatos sociais. Tem um tom mais persuasivo e ganchos de reten√ß√£o mais fortes.",
    "puter/x-ai/grok-2": "[ROBUSTO] (2024) Muito bom para seguir regras r√≠gidas sem 'pular' instru√ß√µes. Segue bem a proibi√ß√£o de humanos nas imagens.",
    "openrouter/deepseek/deepseek-r1-0528:free": "Ideal para l√≥gica rigorosa, revis√£o gramatical avan√ßada e extra√ß√£o de regras complexas, sem as taxas da OpenAI. Menos 'criativo', mas muito preciso nos dados.",
    "openrouter/deepseek/deepseek-r1-0528:free": "[RACIOC√çNIO] (2025) Ideal para calibragem. Pensa passo a passo, identificando erros sutis de pron√∫ncia e tom.",
    "openrouter/google/gemma-3-27b-it:free": "[IMAGEM/VIS√ÉO] (2025) Vers√£o aberta do Google. Surpreendentemente bom em descrever detalhes de fotos do produto.",
    "openrouter/meta-llama/llama-4-scout:free": "[GIGANTE] (2025) Intelig√™ncia de ponta para descri√ß√µes ricas. √ìtimo para quando voc√™ quer um texto mais longo e detalhado.",
    "zai/glm-4-flash": "[PRECIS√ÉO] (2024) IA chinesa focada em n√£o alucinar. Se o produto tem muitos n√∫meros e medidas, ele √© uma √≥tima escolha.",
    "kimi/moonshot-v1-8k": "[COER√äNCIA] (2024) Mant√©m o fio da meada em roteiros longos. Bom para v√≠deos de Review extensos.",
    "puter/meta-llama/llama-3.1-70b-instruct": "[EQUILIBRADO] (2024) Intelig√™ncia de n√≠vel Pro em formato aberto. Vers√°til para todos os modos de trabalho.",
    "puter/claude-3-5-sonnet": "[NARRATIVA PREMIUM] (2024) O rei da escrita natural. Se voc√™ quer que o roteiro pare√ßa escrito por um redator s√™nior, use este.",
}
PROVIDER_KEY_MAP = {
    "gemini": "GEMINI_API_KEY",
    "openai": "OPENAI_API_KEY",
    "puter": "PUTER_API_KEY",
    "openrouter": "OPENROUTER_API_KEY",
    "zai": "ZAI_API_KEY",
    "kimi": "KIMI_API_KEY",
}

def calcular_custo_brl(model_id, tokens_in, tokens_out):
    """Calcula o custo estimado em BRL com base nos tokens consumidos."""
    pricing = PRICING_USD_PER_1M.get(model_id, PRICING_USD_PER_1M["gemini-2.5-flash"])
    custo_usd = (tokens_in / 1_000_000 * pricing["input"]) + (tokens_out / 1_000_000 * pricing["output"])
    return round(custo_usd * USD_TO_BRL, 6)

class RoteiristaAgent:
    def __init__(self, supabase_client=None, model_id="gemini-2.5-flash"):
        self.model_id = model_id
        self.supabase = supabase_client
        self.client_gemini = None
        self.client_openai = None
        self.provider = "gemini"

        if self.model_id.startswith("gemini"):
            self.provider = "gemini"
            api_key = os.environ.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY n√£o encontrada!")
            self.client_gemini = genai.Client(api_key=api_key)
        elif self.model_id.startswith("puter/"):
            self.provider = "puter"
            puter_key = os.environ.get("PUTER_API_KEY")
            if not puter_key:
                raise ValueError("PUTER_API_KEY n√£o encontrada!")
            self.client_openai = OpenAI(
                api_key=puter_key,
                base_url="https://api.puter.com/puterai/openai/v1/"
            )
            self.model_id = self.model_id.replace("puter/", "")
        elif self.model_id.startswith("openai/"):
            self.provider = "openai"
            openai_key = os.environ.get("OPENAI_API_KEY")
            if not openai_key:
                raise ValueError("OPENAI_API_KEY n√£o encontrada!")
            self.client_openai = OpenAI(api_key=openai_key)
            self.model_id = self.model_id.replace("openai/", "")
        elif self.model_id.startswith("openrouter/"):
            self.provider = "openrouter"
            or_key = os.environ.get("OPENROUTER_API_KEY")
            if not or_key:
                raise ValueError("OPENROUTER_API_KEY n√£o encontrada!")
            self.client_openai = OpenAI(
                api_key=or_key,
                base_url="https://openrouter.ai/api/v1"
            )
            self.model_id = self.model_id.replace("openrouter/", "")
        elif self.model_id.startswith("zai/"):
            self.provider = "zai"
            zai_key = os.environ.get("ZAI_API_KEY")
            if not zai_key:
                raise ValueError("ZAI_API_KEY n√£o encontrada!")
            self.client_openai = OpenAI(
                api_key=zai_key,
                base_url="https://api.z.ai/api/paas/v4/"
            )
            self.model_id = self.model_id.replace("zai/", "")
        elif self.model_id.startswith("kimi/"):
            self.provider = "kimi"
            kimi_key = os.environ.get("KIMI_API_KEY")
            if not kimi_key:
                raise ValueError("KIMI_API_KEY n√£o encontrada!")
            self.client_openai = OpenAI(
                api_key=kimi_key,
                base_url="https://api.moonshot.cn/v1"
            )
            self.model_id = self.model_id.replace("kimi/", "")

        # Carrega toda a base de conhecimento est√°tica (Apenas prompts e fon√©tica base)
        self.system_prompt = self._load_file(
            os.path.join(PROJECT_ROOT, ".agents", "system_prompt.txt"), ""
        )
        self.phonetics = self._load_json(
            os.path.join(PROJECT_ROOT, "kb", "phonetics.json"), {}
        )
        # Ouro e Calibragem agora s√£o 100% din√¢micos via Supabase
        self.few_shot_examples = [] 
        
        # Carrega documentos de contexto (.md) da KB
        self.context_docs = self._load_all_md_from_kb()

    def _load_file(self, filepath, fallback):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            return fallback

    def _load_json(self, filepath, fallback):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return fallback

    def _load_all_md_from_kb(self):
        """Carrega todos os .md da pasta kb/ como contexto estrat√©gico."""
        docs = []
        kb_path = os.path.join(PROJECT_ROOT, "kb")
        for md_file in glob.glob(os.path.join(kb_path, "*.md")):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Limita cada doc a 4000 chars para n√£o estourar o contexto
                    docs.append(content[:4000])
            except Exception:
                pass
        return docs

    def _fetch_supabase_context(self):
        """Busca aprendizado din√¢mico no Supabase."""
        sb_parts = []
        if not self.supabase:
            return ""
        
        try:
            # 1. Roteiros Ouro (O "Norte" da Reda√ß√£o - Exemplos de Elite)
            res_ouro = self.supabase.table("nw_roteiros_ouro").select("*").order('criado_em', desc=True).limit(5).execute()
            if res_ouro.data:
                sb_parts.append("\n**REFER√äNCIAS DE ELITE (ESTE √â O PADR√ÉO OURO A SER SEGUIDO):**")
                for r in res_ouro.data:
                    sb_parts.append(f"- Produto: {r['titulo_produto']}\n  Roteiro Perfeito (Target): {r['roteiro_perfeito']}")

            # 2. Ajustes de Persona
            res_pers = self.supabase.table("nw_treinamento_persona_lu").select("*").limit(5).execute()
            if res_pers.data:
                sb_parts.append("\n**AJUSTES DE PERSONA (LI√á√ïES APRENDIDAS):**")
                for p in res_pers.data:
                    sb_parts.append(f"- Pilar: {p['pilar_persona']}\n  Erro Anterior: {p['erro_cometido']}\n  Corre√ß√£o Master: {p['texto_corrigido_humano']}")

            # 3. Novas Regras Fon√©ticas
            res_fon = self.supabase.table("nw_treinamento_fonetica").select("*").execute()
            if res_fon.data:
                sb_parts.append("\n**NOVAS REGRAS DE FON√âTICA (OBRIGAT√ìRIO):**")
                for f in res_fon.data:
                    sb_parts.append(f"- {f['termo_errado']} -> ({f['termo_corrigido']})")
                    
            # 4. Estruturas Aprovadas (Aberturas e Fechamentos/CTAs)
            res_est = self.supabase.table("nw_treinamento_estruturas").select("*").execute()
            if res_est.data:
                sb_parts.append("\n**ESTRUTURAS APROVADAS PARA INSPIRA√á√ÉO (HOOKS E CTAs):**")
                for est in res_est.data:
                    sb_parts.append(f"- [{est['tipo_estrutura']}] {est['texto_ouro']}")
                    
            # 5. Nuances de Linguagem (O que evitar e como melhorar)
            res_nuan = self.supabase.table("nw_treinamento_nuances").select("*").limit(5).order('criado_em', desc=True).execute()
            if res_nuan.data:
                sb_parts.append("\n**NUANCES E REFINAMENTO DE ESTILO (LI√á√ïES DE REDA√á√ÉO):**")
                for n in res_nuan.data:
                    refinamento = f"- EVITE: '{n['frase_ia']}'\n  POR QUE: {n['analise_critica']}"
                    if n.get('exemplo_ouro'):
                        refinamento += f"\n  FORMA IDEAL: '{n['exemplo_ouro']}'"
                    sb_parts.append(refinamento)

            # 6. Mem√≥ria de Calibragem (Li√ß√µes Recentes da Calibragem)
            res_fb = self.supabase.table("nw_roteiros_ouro").select("aprendizado").neq("aprendizado", "null").order('criado_em', desc=True).limit(8).execute()
            if res_fb.data:
                valid_mems = [f for f in res_fb.data if f.get('aprendizado') and f['aprendizado'].strip()]
                if valid_mems:
                    sb_parts.append("\n**LI√á√ïES RECENTES DA CALIBRAGEM (N√ÉO REPITA ESTES ERROS):**")
                    for fb in valid_mems:
                        sb_parts.append(f"- {fb['aprendizado']}")
        except Exception as e:
            print(f"Erro ao buscar contexto no Supabase: {e}")
            
        return "\n".join(sb_parts)

    def _build_context(self):
        """Monta o contexto completo: Prompt + KB Estrat√©gica + Fon√©tica + Few-Shot + Supabase."""
        parts = []

        # 1. System Prompt (Regras de Ouro do Breno)
        if self.system_prompt:
            parts.append(self.system_prompt)

        # 2. Contexto estrat√©gico do mercado brasileiro e persona Lu
        if self.context_docs:
            parts.append("\n**CONTEXTO ESTRAT√âGICO (MERCADO BRASILEIRO E PERSONA LU):**")
            parts.append("Use este conhecimento para adaptar o tom e as refer√™ncias do roteiro:")
            for doc in self.context_docs:
                parts.append(doc)

        # 3. Dicion√°rio de fon√©tica (Est√°tico)
        if self.phonetics:
            parts.append("\n**DICION√ÅRIO DE FON√âTICA BASE (PADR√ÉO):**")
            for sigla, pronuncia in self.phonetics.items():
                parts.append(f"- {sigla} -> ({pronuncia})")

        # 4. Few-Shot Learning (Est√°tico)
        if self.few_shot_examples:
            parts.append("\n**EXEMPLOS HIST√ìRICOS DE REFER√äNCIA:**")
            for ex in self.few_shot_examples:
                parts.append(f"\n--- EXEMPLO: {ex.get('produto', '')} ---")
                parts.append(f"‚ùå TEXTO IA: {ex.get('output_antes_ia_ruim', '')}")
                parts.append(f"‚úÖ COMO O BRENO QUER: {ex.get('output_depois_breno_aprovado', '')}")

        # 5. Aprendizado em Tempo Real (Supabase)
        supabase_context = self._fetch_supabase_context()
        if supabase_context:
            parts.append(supabase_context)

        return "\n".join(parts)

    def gerar_memoria_calibracao(self, ia_text, breno_text):
        """Analisa a diferen√ßa entre o texto da IA e o aprovado, e extrai a 'li√ß√£o'. Usa fallback multi-provedor."""
        prompt = (
            "Voc√™ √© um Analista de Reda√ß√£o Publicit√°ria S√™nior comparando DUAS vers√µes de um roteiro de v√≠deo.\n\n"
            "VERS√ÉO A (Gerada pela IA):\n"
            f"{ia_text}\n\n"
            "VERS√ÉO B (Aprovada pelo Humano / Breno):\n"
            f"{breno_text}\n\n"
            "Sua tarefa: N√£o descreva pequenas trocas de palavras. Extraia o PADR√ÉO T√âCNICO DE ESCRITA que o humano aplicou.\n"
            "Exemplos de padr√µes: 'Encurtar ganchos iniciais', 'Remover termos t√©cnicos complexos', 'Focar no benef√≠cio emocional em vez da ficha t√©cnica', 'Usar tom mais imperativo no fechamento'.\n\n"
            "Responda em NO M√ÅXIMO 1 frase objetiva (m√°ximo 150 caracteres). "
            "Use o formato estrito: 'PADR√ÉO OBSERVADO: [descreva a regra t√©cnica de reda√ß√£o aplicada].'\n"
            "N√ÉO use met√°foras. Seja puramente t√©cnico e direto."
        )

        # üü¢ OP√á√ÉO 1: PUTER (Grok 4.1 Fast ‚Äî Gr√°tis)
        api_key_puter = os.environ.get("PUTER_API_KEY")
        if api_key_puter:
            try:
                from openai import OpenAI as OpenAIClient
                client = OpenAIClient(api_key=api_key_puter, base_url="https://api.puter.com/puterai/openai/v1/")
                response = client.chat.completions.create(
                    model="x-ai/grok-4-1-fast",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3
                )
                print("‚úÖ Mem√≥ria de calibragem gerada via Puter (grok-4-1-fast)")
                return response.choices[0].message.content.replace('\n', ' ').strip()
            except Exception as e:
                print(f"‚ö†Ô∏è Erro Puter Mem√≥ria: {e}")

        # üîµ OP√á√ÉO 2: OPENROUTER (DeepSeek V3 ‚Äî Gr√°tis)
        api_key_or = os.environ.get("OPENROUTER_API_KEY")
        if api_key_or:
            try:
                from openai import OpenAI as OpenAIClient
                client = OpenAIClient(api_key=api_key_or, base_url="https://openrouter.ai/api/v1")
                response = client.chat.completions.create(
                    model="deepseek/deepseek-chat-v3-0324:free",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3
                )
                print("‚úÖ Mem√≥ria de calibragem gerada via OpenRouter (deepseek-v3)")
                return response.choices[0].message.content.replace('\n', ' ').strip()
            except Exception as e:
                print(f"‚ö†Ô∏è Erro OpenRouter Mem√≥ria: {e}")

        # üü° OP√á√ÉO 3: GEMINI (se a key funcionar)
        api_key_gemini = os.environ.get("GEMINI_API_KEY")
        if api_key_gemini:
            try:
                from google.genai import types
                client = genai.Client(api_key=api_key_gemini)
                response = client.models.generate_content(
                    model='gemini-2.5-flash',
                    contents=prompt,
                    config=types.GenerateContentConfig(temperature=0.3)
                )
                print("‚úÖ Mem√≥ria de calibragem gerada via Gemini (2.5-flash)")
                return response.text.replace('\n', ' ').strip()
            except Exception as e:
                print(f"‚ö†Ô∏è Erro Gemini Mem√≥ria: {e}")

        return "Erro: Nenhum provedor dispon√≠vel para gerar mem√≥ria de calibragem."

    def gerar_roteiro(self, scraped_data, modo_trabalho="NW (NewWeb)", mes="MAR", data_roteiro=None, codigo=None, nome_produto=None):
        """Envia a requisi√ß√£o para o Gemini gerar o roteiro. Suporta Multimodal e Modos de Trabalho."""
        context = self._build_context()

        # Verifica se o input tem imagem (novo fluxo do scraper)
        if isinstance(scraped_data, dict):
            text_data = scraped_data.get("text", "")
            images_list = scraped_data.get("images", [])
        else:
            text_data = str(scraped_data)
            images_list = []
            
        # Roteamento b√°sico de Prompt baseado no Modo (Expans√£o Futura)
        diretriz_modo = f"Crie um roteiro focado no formato padr√£o NewWeb (descri√ß√£o rica e completa)."
        
        # INJE√á√ÉO DAS T√ÅTICAS NW LU (M√™s e Cena Obrigat√≥ria)
        if "NW" in modo_trabalho:
            data_str = data_roteiro if data_roteiro else "[DATA_ATUAL]"
            prod_str = nome_produto if nome_produto else "[NOME_DO_PRODUTO_AQUI]"
            cod_str = codigo if codigo else "[C√ìDIGO_AQUI]"
            
            sub_skus_str = f" (Varia√ß√µes/Cores: {sub_skus})" if sub_skus else ""
            video_ref_str = f"\n   V√≠deo Base do Fornecedor: {video_url} (Sugira cortes deste v√≠deo para as imagens quando aplic√°vel)" if video_url else ""
            
            diretriz_modo += (
                f"\n\nüö® REGRA ABSOLUTA DE FORMATA√á√ÉO E ESTRUTURA (NW LU):\n"
                f"1. O TEXTO DEVE COME√áAR COM O CABE√áALHO EXATAMENTE NO FORMATO:\n"
                f"   Cliente: Magalu\n"
                f"   Roteirista: Tiago Fernandes - Data: {data_str}\n"
                f"   Produto: NW LU {mes} {cod_str} {prod_str}{sub_skus_str}{video_ref_str}\n"
                f"2. A CENA 1 (Primeira cena do v√≠deo) DEVE OBRIGATORIAMENTE mostrar a 'Lu' em a√ß√£o, interagindo com o produto ou apresentando-o.\n"
                f"3. A partir da CENA 2, CORTE para imagens do produto. REGRA CR√çTICA DE IMAGEM: √â ESTRITAMENTE PROIBIDO sugerir a√ß√µes humanas nas Colunas de Imagem (ex: 'm√£o segurando o celular', 'pessoa bebendo caf√©', 'cliente usando'). O v√≠deo NW √© feito APENAS com fotos est√°ticas do fornecedor, anima√ß√µes gr√°ficas (GCs) e recortes do v√≠deo oficial. IMAGENS 100% LIMPAS DE HUMANOS."
            )

        if "SOCIAL" in modo_trabalho:
            diretriz_modo = f"ATEN√á√ÉO: Este formato √© para SOCIAL (Reels/TikTok). O roteiro deve ser EXTREMAMENTE curto, din√¢mico e focado em reten√ß√£o nos primeiros 3 segundos."
        elif "3D" in modo_trabalho:
            diretriz_modo = f"ATEN√á√ÉO: Este formato √© para 3D. Foque muito em descrever as texturas, cores exatas, reflexos e √¢ngulos importantes para o time de modelagem."
        elif "Review" in modo_trabalho:
            diretriz_modo = f"ATEN√á√ÉO: Este formato √© um REVIEW. Foque em pr√≥s, contras, uso pr√°tico di√°rio e uma opini√£o direta para quem vai gravar no est√∫dio."

        final_prompt = (
            f"{context}\n\n"
            f"**MODO DE TRABALHO SOLICITADO:** {modo_trabalho}\n"
            f"-> {diretriz_modo}\n\n"
            f"**CONTEXTO DO PRODUTO (INPUT TEXTUAL E/OU VISUAL):**\n{text_data}\n\n"
            f"**INSTRU√á√ÉO FINAL:**\n"
            f"1. Gere o roteiro no FORMATO DE SA√çDA OBRIGAT√ìRIO.\n"
            f"2. ENCARNE A PERSONA DA LU: Seja acolhedora, direta e prestativa. Siga RIGOROSAMENTE as Regras de Ouro do Estilo Breno e o Contexto Estrat√©gico.\n"
            f"3. Se houverem imagens fornecidas, extraia o m√°ximo de detalhes visuais (cor, textura, design) para enriquecer o roteiro.\n"
            f"4. Imite fielmente o estilo dos exemplos APROVADOS.\n"
            f"5. Use 'pra' no lugar de 'para'. Coloque a marca entre v√≠rgulas.\n"
            f"6. **ENRIQUECIMENTO DE CONTEXTO:** Para produtos mundialmente conhecidos, adicione detalhes t√©cnicos ou curiosidades RELEVANTES que n√£o estejam na ficha, MAS sem alongar o roteiro desnecessariamente.\n"
            f"7. **REGRA DE REFER√äNCIA:** Se usar conhecimento interno (item 6) ou dados de 'FONTE EXTERNA', adicione OBRIGATORIAMENTE uma nota com o link oficial no rodap√© do roteiro.\n"
            f"8. **PROIBI√á√ÉO DE SCRIPTS HIPOT√âTICOS:** Se o contexto do produto for insuficiente ou tiver mensagem de erro, N√ÉO gere roteiro hipot√©tico. Responda APENAS: 'ERRO: Dados insuficientes do produto para gera√ß√£o autom√°tica.'"
        )

        if self.client_gemini:
            contents = [final_prompt]
            # Adiciona a lista de imagens se houver
            if images_list:
                from google.genai.types import Part
                for img_dict in images_list:
                    img_bytes = img_dict.get("bytes")
                    img_mime = img_dict.get("mime")
                    if img_bytes and img_mime:
                        contents.append(
                            Part.from_bytes(data=img_bytes, mime_type=img_mime)
                        )

            response = self.client_gemini.models.generate_content(
                model=self.model_id,
                contents=contents,
            )
            roteiro = response.text
            
            # Captura m√©tricas de uso (tokens)
            tokens_in = getattr(response.usage_metadata, 'prompt_token_count', 0) if hasattr(response, 'usage_metadata') else 0
            tokens_out = getattr(response.usage_metadata, 'candidates_token_count', 0) if hasattr(response, 'usage_metadata') else 0
        
        elif self.client_openai:
            messages = [{"role": "user", "content": final_prompt}]
            # Para modelos OpenAI/Puter, o envio de imagens (vision) tem uma estrutura diferente.
            # Como a documenta√ß√£o prim√°ria do Puter para Grok Fast n√£o deixa claro o suporte a imagens,
            # passaremos apenas texto por enquanto, a n√£o ser que o modelo suporte e tenhamos url.
            
            response = self.client_openai.chat.completions.create(
                model=self.model_id,
                messages=messages
            )
            roteiro = response.choices[0].message.content
            
            tokens_in = response.usage.prompt_tokens if hasattr(response, 'usage') else 0
            tokens_out = response.usage.completion_tokens if hasattr(response, 'usage') else 0
        
        else:
            raise Exception("Nenhum cliente LLM configurado v√°lido.")

        custo_brl = calcular_custo_brl(self.model_id, tokens_in, tokens_out)

        return {
            "roteiro": roteiro,
            "model_id": self.model_id,
            "tokens_in": tokens_in,
            "tokens_out": tokens_out,
            "custo_brl": custo_brl
        }

    def _extract_json(self, text):
        """Extrai JSON de uma resposta que pode conter markdown wrappers (```json ... ```)."""
        import re
        # Tenta parsear direto
        try:
            return json.loads(text)
        except (json.JSONDecodeError, TypeError):
            pass
        # Tenta extrair de blocos ```json ... ``` ou ``` ... ```
        match = re.search(r'```(?:json)?\s*\n?({.*?})\s*\n?```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        # Tenta encontrar o primeiro { ... } na resposta
        match = re.search(r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        raise ValueError(f"N√£o foi poss√≠vel extrair JSON da resposta: {text[:200]}")

    def analisar_calibracao(self, original, final, categories_list=[], codigo_original=""):
        """
        Realiza a an√°lise de calibragem de qualidade usando LLMs gratuitos.
        Cadeia de fallback: Puter (Grok 4.1 Fast) ‚Üí OpenRouter (DeepSeek V3) ‚Üí Gemini (2.5 Flash).
        """
        # Define um ID de fallback seguro (o primeiro da lista ou 0)
        fallback_id = categories_list[0]['id'] if categories_list else 1
        # Formata a lista de categorias para o prompt
        cat_str = "\n".join([f"- ID {c['id']}: {c['nome']}" for c in categories_list]) if categories_list else "Gen√©rico (ID 1)"

        sys_prompt = (
            "Voc√™ √© um Editor S√™nior de Reda√ß√£o Publicit√°ria e Especialista em Qualidade Magalu.\n"
            "Sua tarefa √© realizar uma ANALISE T√âCNICA E CIR√öRGICA da calibragem:\n\n"
            "1. COMPARE o Roteiro Original (IA) com o Roteiro Final (Aprovado pelo Humano).\n"
            "2. CALCULE o SCORE (%) de aproveitamento real seguindo esta R√âGUA ORG√ÇNICA MAGALU:\n"
            "   - 100%: Perfeito. O humano fez apenas ajustes de formata√ß√£o, pontua√ß√£o ou troca de conectivos sem alterar a ess√™ncia.\n"
            "   - 85% a 95%: Ajustes de Estilo. O humano melhorou a fluidez, encurtou frases ou trocou jarg√µes por termos mais comerciais.\n"
            "   - 60% a 80%: Mudan√ßa Estrutural. O humano adicionou informa√ß√µes faltantes, reconstruiu a abertura/fechamento ou cortou blocos inteiros.\n"
            "   - Abaixo de 60%: Erro Grave. A IA errou feio o tom de voz, omitiu funcionalidades vitais ou errou o SKU.\n"
            "   ATEN√á√ÉO: Termos presentes no C√ìDIGO SUGERIDO ou NOME DO PRODUTO (ex: 'Aro 26', 'Grau', 'Index') N√ÉO S√ÉO ERROS DA IA, n√£o penalize a nota por eles.\n"
            "3. S√çNTESE DE APRENDIZADO (MEM√ìRIA T√âCNICA): Transforme as edi√ß√µes em DIRETRIZES T√ÅTICAS E IMPERATIVAS DE ESCRITA, INCLUINDO SEMPRE UM EXEMPLO PR√ÅTICO do que foi mudado.\n"
            "   Sua diretriz DEVE ser aplic√°vel a futuros roteiros como uma regra de ouro.\n"
            "   - REGRA ANTI-ALUCINA√á√ÉO: √â ESTRITAMENTE PROIBIDO listar especifica√ß√µes t√©cnicas do produto como se fossem regras de reda√ß√£o (ex: 'Falar que tem freio a disco'). Foque APENAS no ESTILO de escrita.\n"
            "   - REGRA DE LOCALIZA√á√ÉO E CONTEXTO: Se o humano CORTOU ou ADICIONOU um bloco de texto, explique O QUE era, POR QUE cortou e ONDE (em qual cena exata isso ocorreu). \n"
            "     * O roteiro possui uma estrutura l√≥gica (ex: Cena 1 - Abertura, Cena 3 - Features, Pen√∫ltima cena - Conex√µes/Bateria, Fechamento). Mapeie a altera√ß√£o para a cena correspondente.\n"
            "     * Ex: 'Na pen√∫ltima cena, cortou redund√¢ncia sobre cansa√ßo visual pois a tecnologia Frost Free ou Flicker-Free j√° havia sido explicada na primeira metade'.\n"
            "   - ERRADO: 'Breno tirou a palavra X e colocou Y.'\n"
            "   - CERTO: '- Focar no benef√≠cio emocional em vez da ficha t√©cnica (Ex: Trocou \"Possui painel IPS\" por \"Cores vivas de qualquer √¢ngulo\"). - Iniciar o texto com sujeito expl√≠cito (Ex: \"Ela tem\" em vez de \"Tem\").'\n"
            "   Seja curto, grosso e imperativo, mas SEMPRE D√ä EXEMPLOS nas pr√≥prias frases. Use t√≥picos com '-'.\n"
            "4. EXTRAIA O C√ìDIGO DO PRODUTO (SKU): Procure no texto por sequ√™ncias num√©ricas ou o c√≥digo fornecido.\n"
            "5. CATEGORIZE (CR√çTICO): Escolha a melhor categoria da lista abaixo baseada na FUN√á√ÉO PRINCIPAL DO PRODUTO. "
            "N√£o se confunda com funcionalidades extras (ex: um monitor gamer com alto-falante √© 'Informatica / Gamer', e NUNCA '√Åudio'). "
            "Leia o texto com aten√ß√£o para identificar a ess√™ncia do produto.\n"
            "6. FON√âTICA (AUTO-EXTRA√á√ÉO): Se o humano ADICIONOU, CORRIGIU ou REMOVEU pron√∫ncias fon√©ticas (ex: acrescentou '(fl√≠ker fr√≠)' para Flicker-Free), "
            "extraia como regras no campo 'fonetica_regras'. Cada regra tem: "
            "'termo_errado' (a vers√£o sem pron√∫ncia se o humano adicionou, ou a vers√£o com pron√∫ncia ruim), 'termo_corrigido' (a vers√£o final que o humano deixou, ex: Flicker-Free (fl√≠ker fr√≠)), "
            "'exemplo' (frase de contexto). Importante: Capturar casos onde o humano removeu o par√™nteses de pron√∫ncia para deixar o texto mais limpo, E TAMB√âM quando o humano adicionou uma pron√∫ncia essencial que a IA esqueceu. Se N√ÉO houver corre√ß√µes, retorne [].\n"
            "7. ESTRUTURAS (AUTO-EXTRA√á√ÉO): Se o humano MUDOU a ABERTURA (primeira frase) ou o FECHAMENTO (√∫ltima frase), "
            "extraia o texto APROVADO PELO HUMANO no campo 'estrutura_regras'. Cada regra tem: "
            "'tipo' ('Abertura' ou 'Fechamento') e 'texto_ouro' (a frase exata aprovada pelo humano). "
            "Se N√ÉO houve mudan√ßa na abertura/fechamento, retorne lista vazia [].\n"
            "8. PERSONA LU (AUTO-EXTRA√á√ÉO): Se o humano corrigiu o TOM DE VOZ, ESTILO ou VOCABUL√ÅRIO da Lu, "
            "extraia como regras no campo 'persona_regras'. Cada regra tem: "
            "'pilar' (tom, vocabul√°rio, gancho, emo√ß√£o, clareza), 'erro' (o que a IA fez de errado), "
            "'correcao' (como o humano corrigiu) e 'lexico' (palavras-chave ou termos prefer√≠veis identificados na corre√ß√£o). "
            "Se N√ÉO houver corre√ß√µes de persona, retorne lista vazia [].\n\n"
            "LISTA DE CATEGORIAS DISPON√çVEIS:\n"
            f"{cat_str}\n\n"
            "üö® REGRA CR√çTICA DE FORMATA√á√ÉO DE SA√çDA:\n"
            "Voc√™ √© um rob√¥ de extra√ß√£o de dados. Retorne EXCLUSIVAMENTE o conte√∫do JSON abaixo.\n"
            "- N√ÉO use blocos de c√≥digo markdown (```json ... ```).\n"
            "- N√ÉO diga 'Aqui est√° o JSON'.\n"
            "- Inicie com { e termine com }.\n\n"
            "Formato exato:\n"
            "{\n"
            "  \"percentual\": <inteiro 0-100>,\n"
            "  \"aprendizado\": \"<diretrizes t√°ticas de escrita em t√≥picos>\",\n"
            "  \"categoria_id\": <id num√©rico da melhor categoria>,\n"
            "  \"codigo_produto\": \"<c√≥digo encontrado no texto ou o original>\",\n"
            "  \"fonetica_regras\": [{\"termo_errado\": \"...\", \"termo_corrigido\": \"...\", \"exemplo\": \"...\"}],\n"
            "  \"estrutura_regras\": [{\"tipo\": \"Abertura\", \"texto_ouro\": \"...\"}],\n"
            "  \"persona_regras\": [{\"pilar\": \"...\", \"erro\": \"...\", \"correcao\": \"...\", \"lexico\": \"...\"}]\n"
            "}"
        )

        user_prompt = f"--- C√ìDIGO SUGERIDO ---\n{codigo_original}\n\n--- ROTEIRO ORIGINAL (IA) ---\n{original}\n\n--- ROTEIRO FINAL (HUMANO) ---\n{final}"

        # Tenta m√∫ltiplos provedores para garantir a calibragem (OpenRouter [DeepSeek] ‚Üí Puter [Grok] ‚Üí Gemini)
        from openai import OpenAI as OpenAIClient
        
        # üîµ OP√á√ÉO 1: OPENROUTER (DeepSeek V3 ‚Äî Gr√°tis e Superior para L√≥gica)
        api_key_or = os.environ.get("OPENROUTER_API_KEY")
        if api_key_or:
            try:
                print("üîÑ Tentando calibragem via OpenRouter (deepseek-r1)...")
                client = OpenAIClient(api_key=api_key_or, base_url="https://openrouter.ai/api/v1")
                response = client.chat.completions.create(
                    model="deepseek/deepseek-r1-0528:free",
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1
                )
                res = self._extract_json(response.choices[0].message.content)
                print("‚úÖ Calibragem realizada via OpenRouter (deepseek-r1)")
                return self._process_calib_res(res, fallback_id, categories_list, codigo_original, "DeepSeek R1 (via OpenRouter)")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro OpenRouter Calibragem: {e}")

        # üü¢ OP√á√ÉO 2: PUTER (Grok 4.1 Fast ‚Äî Gr√°tis e reserva)
        api_key_puter = os.environ.get("PUTER_API_KEY")
        if api_key_puter:
            try:
                print("üîÑ Tentando calibragem via Puter (grok-4-1-fast)...")
                client = OpenAIClient(api_key=api_key_puter, base_url="https://api.puter.com/puterai/openai/v1/")
                response = client.chat.completions.create(
                    model="x-ai/grok-4-1-fast",
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1
                )
                res = self._extract_json(response.choices[0].message.content)
                print("‚úÖ Calibragem realizada via Puter (grok-4-1-fast)")
                return self._process_calib_res(res, fallback_id, categories_list, codigo_original, "Grok 4.1 Fast (via Puter)")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro Puter Calibragem: {e}")

        # üü° OP√á√ÉO 3: GEMINI (√∫ltimo recurso ‚Äî pode ter key inv√°lida)
        api_key_gemini = os.environ.get("GEMINI_API_KEY")
        if api_key_gemini:
            try:
                print("üîÑ Tentando calibragem via Gemini (2.5-flash)...")
                client = genai.Client(api_key=api_key_gemini)
                response = client.models.generate_content(
                    model='gemini-2.5-flash',
                    contents=user_prompt,
                    config=GenerateContentConfig(
                        system_instruction=sys_prompt,
                        response_mime_type="application/json",
                        temperature=0.1
                    ),
                )
                res = json.loads(response.text)
                print("‚úÖ Calibragem realizada via Gemini (2.5-flash)")
                return self._process_calib_res(res, fallback_id, categories_list, codigo_original, "Gemini 2.5 Flash (via Google)")
            except Exception as e:
                print(f"‚ö†Ô∏è Erro Gemini Calibragem: {e}")

        print("‚ùå FALHA TOTAL: Nenhum provedor de IA conseguiu realizar a calibragem.")
        return {"percentual": 50, "aprendizado": "Erro: Nenhum provedor de IA dispon√≠vel para calibragem.", "categoria_id": fallback_id, "codigo_produto": codigo_original, "modelo_calibragem": "N/A", "fonetica_regras": [], "estrutura_regras": [], "persona_regras": []}

    def _process_calib_res(self, res, fallback_id, categories_list, codigo_original, modelo_calibragem="N/A"):
        """Helper para processar e validar o JSON retornado pelos provedores."""
        # Valida√ß√£o rigorosa do ID de categoria
        returned_id = int(res.get("categoria_id", fallback_id))
        valid_ids = [c['id'] for c in categories_list] if categories_list else []
        final_cat_id = returned_id if returned_id in valid_ids else fallback_id
        
        import re
        sku_raw = str(res.get("codigo_produto", codigo_original))
        # SKUs Magalu tem EXATAMENTE 9 d√≠gitos. Priorizamos encontrar esses blocos.
        skus_found = re.findall(r'\b\d{9}\b', sku_raw)
        # Se n√£o achar blocos isolados, tenta achar qualquer sequ√™ncia de 9 d√≠gitos
        if not skus_found:
            skus_found = re.findall(r'\d{9}', sku_raw)
            
        sku_clean = " ".join(skus_found) if skus_found else re.sub(r'\D', '', sku_raw)
        
        return {
            "percentual": int(res.get("percentual", 50)),
            "aprendizado": res.get("aprendizado", "An√°lise realizada."),
            "categoria_id": final_cat_id,
            "codigo_produto": sku_clean,
            "modelo_calibragem": modelo_calibragem,
            "fonetica_regras": res.get("fonetica_regras", []),
            "estrutura_regras": res.get("estrutura_regras", []),
            "persona_regras": res.get("persona_regras", [])
        }

    def chat_with_context(self, user_query, chat_history=[], supabase_context=None):
        """
        Gera uma resposta conversacional baseada no hist√≥rico de chat e,
        opcionalmente, injeta dados recentes do Supabase (RAG-lite) no prompt.
        """
        system_base = (
            "Voc√™ √© a Lu, a assistente virtual inteligente e especialista em IA da Magalu. "
            "Sua miss√£o √© ajudar a equipe interna exclusivamente com: cria√ß√£o de roteiros de v√≠deo, reda√ß√£o publicit√°ria, an√°lise de qualidade (calibragem) e d√∫vidas sobre esta su√≠te de IA. "
            "REGRA DE OURO M√ÅXIMA: √â PROIBIDO responder perguntas fora do contexto da Magalu, tecnologia em varejo, reda√ß√£o ou sobre o sistema de roteiros. Se o assunto sair disso, responda educadamente que voc√™ s√≥ pode ajudar com demandas de conte√∫do da Magalu. "
            "Tenha um tom acolhedor ('estilo magalu'), direto ao ponto, e use emojis ocasionalmente.\n\n"
        )
        
        if supabase_context:
            system_base += f"--- CONTEXTO ATUAL DO BANCO DE DADOS ---\n{supabase_context}\n---------------------------------------\n"

        try:
            if self.provider == "gemini":
                # Para o Gemini (SDK v1), montaremos a interface como um string prompt 
                # contendo o system prompt + hist√≥rico + pergunta
                full_prompt = system_base + "\n\n--- HIST√ìRICO RECENTE ---\n"
                for msg in chat_history[-6:]: 
                    r = msg.get('role', 'user').upper()
                    c = msg.get('content', '')
                    full_prompt += f"{r}: {c}\n"
                full_prompt += f"\nUSU√ÅRIO: {user_query}\nLU:"
                
                response = self.client_gemini.models.generate_content(
                    model=self.model_id,
                    contents=full_prompt,
                    config={"temperature": 0.5}
                )
                return response.text
                
            elif self.provider in ["openai", "puter", "openrouter", "zai", "kimi"]:
                messages = [{"role": "system", "content": system_base}]
                for msg in chat_history[-6:]:
                    r = "assistant" if msg.get("role") == "Lu" else "user"
                    messages.append({"role": r, "content": msg["content"]})
                    
                messages.append({"role": "user", "content": user_query})
                
                response = self.client_openai.chat.completions.create(
                    model=self.model_id,
                    messages=messages,
                    temperature=0.7
                )
                return response.choices[0].message.content
                
            else:
                return "Provedor LLM n√£o reconhecido para Chat."
                
        except Exception as e:
            return f"Desculpe, tive um problema t√©cnico ao conectar com a IA ({self.model_id}): {e}"
